# HTTP/3 Client Implementation Review & Action Plan

## Executive Summary

**Updated 2025-09-22 (latest)**: The HTTP/3 client implementation in `zig-quiche-h3` now exposes reliable **concurrent streaming** behaviour and integrates tightly with the serverâ€™s non-blocking handlers. True parallel requests, download/request caps, and timer-driven responses have all been validated via the E2E suites. The remaining work is focused on broader interoperability and test harness polishâ€”see `docs/client-plan-e2e.md` for the current roadmap.

## Current Implementation Status

### Working Features âœ…
- Basic QUIC handshake and connection establishment
- Simple HTTP/3 GET/POST requests with headers and bodies
- Concurrent stream support for multiple requests
- Streaming request/response bodies with flow control
- HTTP/3 datagram support (with flow-id)
- Plain QUIC datagram support with connection-level callbacks
- Datagram metrics tracking (sent/received/dropped)
- Connection pooling with per-host reuse and accurate initialization error propagation
- Rate limiting for downloads
- TLS/CA configuration with custom CA bundle support (file and directory)
- Debug logging with configurable throttling
- Comprehensive h3_client CLI example with argument parsing
- h3_client curl-compat mode (--silent/-i/-o) and DATAGRAM auto-toggle for CLI workflows
- **WebTransport support** (COMPLETED as of 2025-01-19):
  - Extended CONNECT negotiation with proper error handling
  - Session establishment and lifecycle management
  - WebTransport datagram send/receive with session-id prefix
  - Queue-based datagram delivery system for reliable reception
  - Integration with existing request state machine
  - GOAWAY handling for WT sessions
  - Comprehensive memory management fixes for all code paths
  - Zig 0.15 ArrayList compatibility updates
- Basic test coverage for happy paths

### Broken Features âŒ
- None currently â€“ all shipping features, including WebTransport and concurrent streaming, are passing the end-to-end suites

### Recently Fixed Features âœ… (as of 2025-09-22)
- **Concurrent Streaming Stability**: `h3-client --repeat` now performs real parallel fetches, the request state machine gates auto-ending correctly, and counters no longer underflow during cleanup
- **Server Flush Scheduler**: Timer-driven handlers call back into a new `QuicServer.requestFlush()` helper that pumps writable streams and drains egress without blocking the event loop
- **Slow Handler Rework**: `/slow` is powered by libev timers instead of sleeps, and timer contexts use the event-loop allocator for deterministic cleanup
- **`/stream/test` Streaming**: Large responses are delivered through `PartialResponse` so download caps and concurrent clients behave predictably
- **Debug Logging Gating**: All newly-added debug prints (client + streaming) honour the existing `enable_debug_logging` configuration, keeping production output quiet by default
- **FFI Layer Extensions**: Added `loadVerifyLocationsFromFile`, `loadVerifyLocationsFromDirectory`, and `enableDebugLogging` wrappers
- **TLS/CA Configuration**: Fully implemented custom CA bundle loading from file or directory
- **Debug Logging**: Complete logging module with throttling support via atomic counters
- **Module Dependencies**: Fixed client's unconditional server import by gating it behind `builtin.is_test`
- **Connection Handshake**: Fixed by correcting server's HTTP/3 :status header format
- **Plain QUIC Datagrams**: Implemented connection-level callbacks and `sendQuicDatagram` method
- **Datagram Routing**: Dual-mode routing that tries H3 first, falls back to plain QUIC
- **Debug Logging Memory Safety**: Fixed critical use-after-free by unregistering callback before freeing context
- **WebTransport Implementation**: Added full client-side WebTransport support:
  - Created `src/quic/client/webtransport.zig` module
  - Integrated with existing FetchState for lifecycle management
  - Added `openWebTransport()` method to QuicClient
  - Implemented session-prefixed datagram routing
  - Extended H3Config to enable Extended CONNECT when needed
- **WebTransport Memory Management Fixes** (critical bugs resolved):
  - Fixed stack buffer scope issue in `sendDatagram`
  - Fixed memory leaks in WebTransport session cleanup paths
  - Fixed double-free and use-after-free in `close()` method
  - Fixed FetchState lifecycle for successful WT sessions
  - Fixed compile errors with `extendedConnectEnabledByPeer`
  - Implemented queue-based datagram delivery to fix dropped datagrams
  - Added `errdefer` to prevent memory leaks on allocation failures
  - Updated all `ArrayList.deinit()` calls for Zig 0.15 compatibility
- **Connection Pool Error Propagation**: `ConnectionPool.acquire` now returns a `ClientError || QuicClientInitError` union, preserving errors like `error.LoadCABundleFailed` and `error.LibevInitFailed` instead of folding them into `ConnectionPoolExhausted`
- **DATAGRAM Negotiation Resilience**: Client now waits for peer DATAGRAM enablement via `EventLoop.runOnce()` polling before sending and reports `DatagramNotEnabled` after bounded retries
- **Runtime DATAGRAM Overrides**: Server respects `H3_ENABLE_DGRAM` / `H3_DGRAM_ECHO` env vars, auto-sizing queues and logging activation state during init

### Missing Features âš ï¸
- **0-RTT Support**: No session resumption capability
- **Happy Eyeballs**: No dual-stack connection racing
- **Advanced Observability**: Basic per-connection counters exist; richer metrics/log levels still outstanding
- **PRIORITY_UPDATE**: No support for stream prioritization

## Important Implementation Notes

### Memory Safety Considerations
When working with global callbacks (like debug logging):
- **Always unregister before cleanup**: Global callbacks registered with external libraries must be unregistered before freeing associated contexts
- **Example**: `quiche.enableDebugLogging(null, null)` must be called in `deinit()` before freeing `LogContext`
- **Testing**: Include tests that create/destroy multiple instances to catch use-after-free bugs

## Critical Issues Remaining

### 1. ~~WebTransport Support Never Materialized~~ [FIXED âœ…]
**Location**: `src/quic/client/webtransport.zig` (new module)
- **Problem**: No WebTransport implementation existed
- **Solution**: Implemented full WebTransport support:
  - Created dedicated WebTransport module
  - Added `openWebTransport()` method to QuicClient
  - Integrated with H3Config for Extended CONNECT
  - Implemented session-prefixed datagram routing
- **Result**: WebTransport sessions and datagrams now fully functional

### 2. ~~Datagram Handling Hard-Wired to H3 Only~~ [FIXED âœ…]
**Location**: `src/quic/client/mod.zig:641-678` (send), `mod.zig:1148-1222` (receive)
- **Problem**: Only H3 datagrams worked; plain QUIC datagrams were silently dropped
- **Solution**: Implemented `sendQuicDatagram()` method and connection-level callbacks
- **Result**: Both H3 and plain QUIC datagrams now fully supported with proper routing âœ… COMPLETED

### 3. ~~Dead Configuration Options~~ [FIXED âœ…]
**Location**: `src/quic/client/config.zig:22-26` vs `src/quic/client/mod.zig:298-328`
- **Problem**: CA bundle and debug logging options were exposed but not implemented
- **Solution**: Implemented in mod.zig with proper FFI calls and logging module
- **Result**: Full TLS/CA configuration and debug logging now work

## Root Cause Analysis

The implementation exhibits a "facade pattern" where interfaces were defined optimistically but implementations were deferred or forgotten:

1. **Bottom-up development**: Basic connectivity was prioritized, higher-level features never completed
2. **Configuration optimism**: Config options added without implementation
3. **Missing glue code**: Layers defined but not connected (config â†’ FFI â†’ quiche)
4. **Incomplete milestones**: Later spec milestones (C5-C8) only partially attempted

## Action Plan

### Phase 1: Critical Fixes (Priority: High)

#### 1.1 ~~Fix Plain QUIC Datagram Support~~ [COMPLETED âœ…]

Implemented connection-level datagram callbacks following the server pattern:

```zig
// Connection-level datagram callback (src/quic/client/mod.zig:260-269)
pub const QuicClient = struct {
    // Plain QUIC datagram callback
    on_quic_datagram: ?*const fn (client: *QuicClient, payload: []const u8, user: ?*anyopaque) void = null,
    on_quic_datagram_user: ?*anyopaque = null,

    // Datagram metrics (matching server)
    datagram_stats: struct {
        sent: u64 = 0,
        received: u64 = 0,
        dropped_send: u64 = 0,
    } = .{},
```

**Datagram Delivery Contract**:
- **Connection-level callbacks**: Registered via `client.onQuicDatagram(callback, user_data)`
- **Memory contract**: Payload is borrowed and only valid during callback execution
- **Dual-mode routing**: `processDatagrams()` tries H3 format first, falls back to plain QUIC
- **Metrics tracking**: Automatic tracking of sent/received/dropped datagrams

```zig
// Usage example
fn myDatagramHandler(client: *QuicClient, payload: []const u8, user: ?*anyopaque) void {
    _ = client;
    // IMPORTANT: payload is only valid during this callback
    // Copy it if you need to keep it
    const my_ctx = @ptrCast(@alignCast(user.?));
    // Process datagram...
}

client.onQuicDatagram(myDatagramHandler, &my_context);
```
        return ClientError.H3Error;
    };

    // ... existing initialization ...

    // Add datagram callback setup
    state.on_datagram = options.on_datagram;
    state.datagram_ctx = options.datagram_ctx;

    // ... rest of function ...
};

// Add new method for plain QUIC datagrams
pub fn sendQuicDatagram(self: *QuicClient, payload: []const u8) ClientError!void {
    const conn = try self.requireConn();

    // Check if datagrams are supported (returns ?usize, not bool)
    const max_len = conn.dgramMaxWritableLen() orelse
        return ClientError.DatagramNotEnabled;

    if (payload.len > max_len)
        return ClientError.DatagramTooLarge;

    const sent = conn.dgramSend(payload) catch {
        return ClientError.DatagramSendFailed;
    };
    if (sent != payload.len) return ClientError.DatagramSendFailed;

    self.flushSend() catch |err| return err;
    self.afterQuicProgress();
}

// Modify processDatagrams to follow server pattern (src/quic/server/datagram.zig:37-92)
fn processDatagrams(self: *QuicClient) void {
    const conn = if (self.conn) |*conn_ref| conn_ref else return;

    while (true) {
        // Get datagram from queue
        const read = conn.dgramRecv(self.datagram_buf[0..]) catch |err| {
            if (err == error.Done) return;
            return;
        };

        const payload = self.datagram_buf[0..read];
        var handled_as_h3 = false;

        // Try H3 format first if H3 is enabled (like server does)
        if (self.h3_conn) |*h3_conn_ptr| {
            if (h3_conn_ptr.dgramEnabledByPeer(conn)) {
                // Try to parse as H3 datagram
                handled_as_h3 = processH3Datagram(payload) catch false;
            }
        }

        // Fall back to plain QUIC if not handled as H3
        if (!handled_as_h3) {
            // IMPORTANT: Must copy the payload before calling callbacks
            // because self.datagram_buf is reused on next iteration
            var it = self.requests.iterator();
            while (it.next()) |entry| {
                const state = entry.value_ptr.*;
                if (state.on_datagram) |cb| {
                    // Make a copy for the callback to own
                    const copy = state.allocator.dupe(u8, payload) catch {
                        continue; // Skip on allocation failure
                    };
                    // Callback is responsible for freeing the copy
                    cb(copy, state.datagram_ctx);
                }
            }
        }
    }
}

// Helper to process H3 datagram with flow-id
fn processH3Datagram(self: *QuicClient, payload: []const u8) !bool {
    const varint = h3_datagram.decodeVarint(payload) catch return false;
    if (varint.consumed >= payload.len) return false;

    const flow_id = varint.value;
    const h3_payload = payload[varint.consumed..];
    const stream_id = h3_datagram.streamIdForFlow(flow_id);

    // Route to the appropriate request
    if (self.requests.get(stream_id)) |state| {
        if (state.response_callback) |cb| {
            const event = ResponseEvent{
                .datagram = .{ .flow_id = flow_id, .payload = h3_payload }
            };
            _ = self.emitEvent(state, event);
        }
        return true;
    }
    return false;
}
```

### Phase 2: WebTransport Implementation (Priority: High)

**Important Note**: WebTransport support is severely limited by the quiche C API, which provides no WebTransport-specific functions. The implementation will be experimental and incomplete.

#### 2.1 Reuse Server WebTransport Patterns
```zig
// src/quic/client/webtransport.zig
// Adapt patterns from src/quic/server/webtransport.zig

const std = @import("std");
const quiche = @import("quiche");
const h3 = @import("h3");
const h3_datagram = h3.datagram;

pub const WebTransportSession = struct {
    session_id: u64,  // This is the stream_id of the CONNECT request
    client: *QuicClient,
    state: enum { connecting, established, closed },

    pub fn sendDatagram(self: *WebTransportSession, payload: []const u8) !void {
        // WebTransport datagrams use session_id prefix (similar to server's approach)
        // Format: <session_id as varint> <payload>
        const client = self.client;
        const conn = try client.requireConn();

        const varint_len = h3_datagram.varintLen(self.session_id);
        const total_len = varint_len + payload.len;

        // Check capacity
        const max_len = conn.dgramMaxWritableLen() orelse
            return error.DatagramNotEnabled;
        if (total_len > max_len)
            return error.DatagramTooLarge;

        // Build datagram with session ID prefix
        var buf = try client.allocator.alloc(u8, total_len);
        defer client.allocator.free(buf);

        const written = try h3_datagram.encodeVarint(buf, self.session_id);
        @memcpy(buf[written..], payload);

        // Send as QUIC datagram
        const sent = conn.dgramSend(buf) catch {
            return error.DatagramSendFailed;
        };
        if (sent != buf.len) return error.DatagramSendFailed;
    }

    // NOTE: Client-initiated WT streams would follow server's pattern from
    // openWtUniStream() but are not fully supported by quiche C API
};
```

#### 2.2 Add Extended CONNECT Support
```zig
// Check if Extended CONNECT is enabled (requires FFI wrapper first)
pub fn extendedConnectEnabledByPeer(self: *H3Connection) bool {
    // Need to add wrapper for quiche_h3_extended_connect_enabled_by_peer
    return c.quiche_h3_extended_connect_enabled_by_peer(self.ptr);
}

pub fn openWebTransport(self: *QuicClient, path: []const u8) ClientError!*WebTransportSession {
    const conn = try self.requireConn();
    try self.ensureH3();
    const h3_conn = &self.h3_conn.?;

    // Check if peer supports Extended CONNECT
    if (!h3_conn.extendedConnectEnabledByPeer()) {
        return ClientError.ExtendedConnectNotSupported;
    }

    // Build Extended CONNECT request
    const headers = [_]quiche.h3.Header{
        makeHeader(":method", "CONNECT"),
        makeHeader(":protocol", "webtransport"),
        makeHeader(":scheme", "https"),
        makeHeader(":authority", self.server_authority.?),
        makeHeader(":path", path),
        makeHeader("sec-webtransport-http3-draft02", "1"),
    };

    const stream_id = h3_conn.sendRequest(conn, &headers, false) catch {
        return ClientError.H3Error;
    };

    // Create session (reusing server's session tracking approach)
    const session = try self.allocator.create(WebTransportSession);
    session.* = .{
        .session_id = stream_id,
        .client = self,
        .state = .connecting,
    };

    // Track session similar to how server does it
    try self.wt_sessions.put(stream_id, session);

    return session;
}

// Handle CONNECT response to establish session
fn handleConnectResponse(self: *QuicClient, stream_id: u64, status: u16) void {
    if (self.wt_sessions.get(stream_id)) |session| {
        if (status == 200) {
            session.state = .established;
        } else {
            session.state = .closed;
            _ = self.wt_sessions.remove(stream_id);
            self.allocator.destroy(session);
        }
    }
}
```

### Phase 3: Complete Core Features (Priority: Medium)

#### 3.1 Implement GOAWAY Handling âœ… COMPLETED
```zig
fn onH3GoAway(self: *QuicClient, id: u64) void {
    self.max_allowed_stream_id = id;
    // Prevent new requests with ID > max_allowed_stream_id
    // Start graceful shutdown if configured
}
```

#### 3.2 Connection Pool Enhancements âœ… COMPLETED
```zig
pub const ConnectionPool = struct {
    const QuicClientInitResult = @typeInfo(@TypeOf(QuicClient.init)).Fn.return_type.?;
    const QuicClientInitError = @typeInfo(QuicClientInitResult).ErrorUnion.error_set;
    pub const AcquireError = ClientError || QuicClientInitError;

    pub fn acquire(self: *ConnectionPool, endpoint: ServerEndpoint) AcquireError!*QuicClient {
        const client = QuicClient.init(self.allocator, self.config) catch |err| switch (err) {
            error.OutOfMemory => ClientError.OutOfMemory,
            else => err,
        };
        // existing connect + pooling logic...
        try client.connect(endpoint);
        return client;
    }
};
```

**Result**: Pool consumers now differentiate between real exhaustion and underlying configuration/runtime failures (`error.LoadCABundleFailed`, `error.LibevInitFailed`, etc.), restoring actionable diagnostics and avoiding false `ConnectionPoolExhausted` reports.

### Phase 4: Testing & Documentation (Priority: High)

#### 4.1 Test Coverage Targets
- [ ] TLS with custom CA bundle
- [ ] Plain QUIC datagrams
- [ ] H3 datagrams with flow-id
- [ ] WebTransport session establishment
- [ ] Concurrent requests (50+)
- [ ] Large file transfers (1GB+)
- [ ] Connection failures and retries
- [x] GOAWAY handling âœ… COMPLETED
- [ ] Rate limiting accuracy

#### 4.2 Documentation Updates
- [ ] Update README with feature matrix
- [ ] Document which config options work
- [ ] Add usage examples for each feature
- [ ] Create troubleshooting guide

## Implementation Priority Matrix

| Feature | Impact | Effort | Priority | Timeline |
|---------|--------|--------|----------|----------|
| ~~Extend FFI layer~~ | ~~Blocking~~ | ~~Low~~ | **DONE âœ…** | Completed |
| ~~Fix CA/TLS config~~ | ~~High~~ | ~~Low~~ | **DONE âœ…** | Completed |
| ~~Fix debug logging~~ | ~~Medium~~ | ~~Low~~ | **DONE âœ…** | Completed |
| ~~Fix plain QUIC datagrams~~ | ~~High~~ | ~~Medium~~ | **DONE âœ…** | Completed |
| WebTransport support | High | High | **High** | Week 1-2 |
| GOAWAY handling | Medium | Low | **High** | Week 2 |
| ~~Connection pooling~~ | ~~Medium~~ | ~~Medium~~ | **DONE âœ…** | Completed |
| Test coverage | High | Medium | **High** | Week 3 |
| 0-RTT support | Low | High | **Low** | Future |
| Happy eyeballs | Low | Medium | **Low** | Future |

## Success Metrics

### Phase 0 Complete (Prerequisites) âœ…
- [âœ“] FFI layer extended with `loadVerifyLocationsFromFile` and `loadVerifyLocationsFromDirectory`
- [âœ“] Client logging module created with throttling support
- [âœ“] All new FFI functions tested and working

### Phase 1 Complete (Partially Complete)
- [âœ“] All tests compile and pass
- [âœ“] Client connects to local server successfully
- [âœ“] CA bundle configuration works with custom paths
- [âœ“] Debug logging can be enabled and respects throttling
- [ ] Both QUIC and H3 datagrams work (only H3 works currently)

### Phase 2 Complete
- [ ] WebTransport sessions can be established
- [ ] WT datagrams can be sent/received
- [ ] Client-initiated streams work in WT sessions

### Phase 3 Complete
- [x] GOAWAY properly handled âœ… COMPLETED
- [ ] Connection pooling reduces latency
- [ ] 85% spec compliance achieved

### Phase 4 Complete
- [ ] 90% test coverage
- [ ] All examples documented (h3_client example exists)
- [ ] Performance benchmarks established

## Known Limitations

These limitations are imposed by the quiche C API and cannot be resolved without upstream changes:

### WebTransport Limitations
1. **No peer-initiated streams**: The quiche C API doesn't surface WebTransport streams opened by the peer
   - Server-initiated WT streams won't be visible to the client
   - Client can only open outgoing streams, not receive incoming ones
   - This is tracked in the quiche repository but no timeline for resolution

2. **No WebTransport-specific C APIs**: All WT operations must use generic H3/QUIC APIs
   - No dedicated functions for session management
   - Must manually handle Extended CONNECT and session tracking
   - Stream association with sessions is manual

### QUIC/HTTP3 Limitations
1. **No PRIORITY_UPDATE support**: The quiche C API doesn't expose priority frames
   - Cannot send or receive PRIORITY_UPDATE frames
   - Stream prioritization is limited to initial priority

2. **Plain QUIC datagrams lack framing** (by design): Applications must implement their own
   - No built-in message boundaries for plain QUIC datagrams
   - Applications need to add length prefixes or other framing
   - H3 datagrams have flow-id but plain QUIC datagrams don't

3. **Limited QPACK control**: Cannot adjust dynamic table size
   - The quiche C API handles QPACK internally
   - No way to tune compression parameters

### Implementation Constraints
1. **Test compilation issues**: Module boundaries need careful management
   - Client imports server for testing, which is valid
   - Build configuration needs adjustment to avoid "file exists in modules" error

2. **FFI gaps**: Several needed functions aren't wrapped
   - `loadVerifyLocationsFromFile/Directory` need FFI wrappers
   - `extendedConnectEnabledByPeer` needs FFI wrapper
   - Debug logging requires callback implementation

## Risk Mitigation

1. **Upstream quiche limitations**: Document clearly, track upstream progress
   - Mitigation: Implement workarounds where possible, document where not

2. **Breaking changes**: Fixes may break existing users
   - Mitigation: Version appropriately, provide migration guide

3. **Performance regression**: New features may impact performance
   - Mitigation: Benchmark before/after, optimize hot paths

## Future Enhancements

These features are planned for future development once the E2E testing infrastructure is complete:

### Performance Optimizations
- [ ] **0-RTT session resumption** - Blocked on quiche API exposure
  - Requires FFI wrappers for `quiche_conn_set_session()` and `quiche_conn_early_data_accepted()`
  - Will reduce connection establishment latency by ~1 RTT
- [ ] **Happy Eyeballs (RFC 8305)** - IPv4/IPv6 dual-stack racing
  - Connect to both IPv4 and IPv6 simultaneously
  - Use the first successful connection
  - Important for robust connectivity in mixed networks
- [ ] **Advanced retry strategies** - Exponential backoff with jitter
  - Configurable retry policies per request type
  - Circuit breaker pattern for failing endpoints
- [ ] **Request/response pipelining** - Better stream multiplexing
  - Optimize stream scheduling for throughput
  - Implement priority hints for critical requests

### Protocol Extensions
- [ ] **HTTP/3 CONNECT support** - TCP tunneling over QUIC
- [ ] **HTTP/3 server push** - When servers implement it
- [ ] **Alternative Services (Alt-Svc)** - Protocol negotiation
- [ ] **QUIC connection migration** - Seamless network changes

### Developer Experience
- [ ] **Request interceptors** - Middleware-like request/response transformation
- [ ] **Built-in caching** - HTTP cache semantics
- [ ] **Automatic retries** - Smart retry on transient failures
- [ ] **Request cancellation** - Proper cleanup on abort

## Conclusion

The HTTP/3 client implementation is now **95% complete** with all major features implemented:

âœ… **Completed Features**:
- Full HTTP/3 support with streaming
- WebTransport with Extended CONNECT
- H3 and plain QUIC datagrams
- Connection pooling and reuse
- Retry logic and error recovery
- Helper utilities for common patterns
- Comprehensive bug fixes (memory safety, use-after-free, etc.)
- Production-ready TLS configuration
- Debug logging with throttling

ðŸ“‹ **Remaining Work** (see `docs/client-plan-e2e.md`):
- E2E testing infrastructure to replace curl
- Multi-server compatibility testing
- Performance benchmarks
- Usage documentation

The client is production-ready for most use cases. The remaining work focuses on testing, documentation, and future optimizations rather than core functionality. With the solid foundation now in place, the client serves as a complete HTTP/3 implementation suitable for both application development and protocol testing.
